# 刘菁菁-大数据开发/数仓工程师

## 介绍

![user-3](./assets/user-3.png)  刘菁菁|26岁|女|4年工作经验    ![wechat](./assets/wechat-2.png)  miao677 <img src="./assets/mmexport1631948578362.jpg" align="right" width = "200" height = "300" alt="图片名称" />

![envelope](./assets/envelope.png)  xss562646310@gmail.com     ![phone-call](./assets/phone-call.png)  17620439967 

![height-3](./assets/height-3-3989444.png) 172cm|60kg|未婚未育|在职     ![location](./assets/location.png)  广东|深圳



## 教育

2016 - 2020    广东石油化工学院    全日制本科

## 技能

<kbd>编程语言</kbd> 

- [x] [java]()
- [x] [scala]()
- [x] [shell]()
- [x] [sql]()

<kbd>大数据处理框架</kbd>

- 熟悉`Hadoop`生态圈，HDFS底层原理，熟练掌握集群搭建、配置、HDFS文件管理和分布式计算。
- 熟悉`MapReduce`工作原理与执行流程，，熟悉shuffle机制和MapReduce调优。
- 了解`Yarn`的工作原理和调度策略，熟练使用Yarn常用命令。
- 熟悉`Hive`内部表、外部表、分桶表、UDF函数、数据倾斜问题解决，熟悉HiveSQL和Hive调优。
- 熟悉`Hbase`的读写工作原理，熟悉RowKey的设计原则和热点问题的解决。
- 熟悉`Mysql`sql语法和表格管理。

<kbd>分布式计算框架</kbd>

- 熟悉`Spark`执行原理，熟练使用Spark-Core常用算子和SparkSQL语言解决数据计算问题，熟练解决Spark中数据倾斜问题与调优。
- 熟悉`Flink API`和`FlinkSQL`进行实时数据流处理和报表管理，了解Flink状态管理和容错机制。
- 熟悉`Flink-CDC`实时同步。

<kbd>其他技术组件</kbd>

- 熟练使用`Flume`进行数据采集，能够针对业务场景自定义拦截器和组件配置。
- 熟练使用`DataX/Sqoop`进行数据迁移。
- 了解`Atlas`元数据管理系统。
- 熟悉`DolphinScheduler`的任务调度。
- 熟悉`kafka`架构和读写工作原理，了解Kafka-eagle的Web监控使用。
- 熟悉`Doris`各类索引和rollup的配置使用
- 熟悉`Zookeeper`对集群的通讯控制和选举机制。

## 成就

深圳市众乐互娱 2022年度优秀员工

## 公司

| 时间              | 公司名称                   | 职位             |
| ----------------- | -------------------------- | ---------------- |
| 2022.07 - 至今    | 河北宅喵网络科技有限公司   | 大数据开发工程师 |
| 2021.05 - 2022.06 | 深圳市众乐互娱科技有限公司 | PM&数据分析      |
| 2020.06 - 2021.05 | 深圳市乐玩电竞科技有限公司 | 大数据开发工程师 |

## 项目

#### 猫耳FM实时数据流处理平台搭建

2023.03 - 至今	大数据开发

##### 技术架构：

OpenResty + Flume + MySQL + Canal+ Kafka + Flink + RocksDB + HBase + Drools + Redis + Doris

##### 项目简介：

Eagle 实时智能营销系统是一个实时智能‘动态规则’计算引擎，核心是根据动态规则、标签、历史行为条件筛选出符合条件的用户，为实时推送系统提供数据支持，对用户进行精准定位，最终实现实时推送短信和优惠券的功能。它可以帮助运营人员高效、便捷地进行实时营销规则的动态部署，计算并筛选出目标用户，实现精准触达，从而增 加用户粘度和营销转化率，保证了数据宝贵的时效价值

##### 项目存储系统：

1. MySQL：主要用于存储规则数据 
2. Hbase：存储用户画像的标签数据 
3. Flink 状态 ：存储近期用户行为事件数据 
4. Redis：用作缓存 
5. Doris：存储用户行为事件数据

##### 项目流程：

1. 数据接入：利用 Flume 采集用户行为日志到 Kafka 中 

2. 规则管理模块：运营人员通过前端界面制定营销规则，通过 FreeMarker 模板引擎生成 drl 文件存入 MySQL 中

3. 规则注入模块：将规则数据从 Kafka 读取到营销系统中，利用 Drools 规则引擎的 drl 文件应对的 KieSession 加载定义好的规则，利用广播状态进行广播数据流 

4. 动态分组模块：根据不同在线规则按照不同的字段进行分组，对日志数据进行复制和标记分组的 Key 
5. 规则计算模块：将分组后的用户日志数据和广播规则两个流进行 connect，进入 processFunction，在 processFunction 中获得所有在线规则，遍历所有符合分组字段的规则，对输入的用户当前行为事件进行规则计算（匹配触发条件、用户画像标签、用户历史行为序列和用户历史行为次数、时间条件） 

6. 查询缓存系统：将查询结果缓存起来，较少对 Doris 的查询压力 
7. 数据输出：输出筛选出来的当前用户数据及对应规则到上层应用实时推送系统所对接的 Kafka 消息缓存

 

#### 猫耳FM用户画像与标签系统开发

2022.07 - 至今	大数据开发

##### 技术架构：

HBase + MySql + Spark + Hive + Flume + Zookeeper

##### 项目简介：

基于用户行为数据而获取的用户形式化表达等需求创建和维护的数字化模块，通过用户行为日志数据和业务库数据对用户进行多度量和多维度分析，打造用户的自然属性、商业属性、标签属性、行为属性构建画像标签，实现从客户细分、营销策划、营销执行到效果评估的精准营销闭环管理，提升非付费用户到付费客户的转换率，提高用户忠诚度。

##### 标签设计：

1. 统计类标签：基于业务系统和数仓系统的数据，通过 Hive/Spark统计得出，如客户基本属性标签、产品和服务购买历史、消费订单统计标签等，结果标签表存放在DWS层。

2. 规则类标签：基于统计标签进行规则权重打分，然后再按规则分值的阈值区间判断，输出枚举值 如: 客户粘性级别、客户活跃级别、客户生命周期、客户价值等级等级等标签，根据这些标签判断出客户对公司带来的潜在利润， 筛选有质量的客户。

3. 挖掘类标签: 基于用户的行为数据结合统计类标签和规则类标签以及属性标签经过自定义的特殊算法进一步对用户进行打标签，例如:客户套路倾向、客户社会关系网、客户流失风险等级、促销敏感度、客户套路倾向、客户态度与观点、客户行为偏好信息等，为个性化推荐提供数据支撑。

##### 数据输出：

1. 将标签数据使用 BulkLoad 从 Hive 导入到 HBase，服务于推荐系统和营销系统，进行个性化推荐和营销 

2. 是使用分析平台对接 Hive 的标签结果表进行数据统计与分析、数据可视化

##### 项目技术模型：

1. 客户细分模型 - 对不同数据样本分析群体特征，划分高价值和非高价值客户群，并根据营销效果进行持续优化 

2. 付费客户画像 - 提取不同样本群差异化特征维度，针对差异化维度进行建模测试，训练并达到预期建模效果 

3. 构建客户的忠诚度模型 - 通过提取的事实标签和规则标签，构建cube概览表划分用户价值 

4. 受众群体的扩散模型构建 - 运用已购买客户共同特征，筛选最具购买倾向的客户名单 

#### 猫耳FM离线数仓开发与维护

2022.07 - 至今  大数据开发

##### 技术架构：

hdfs + yarn + hbase + mysql + kafka + flume + flink-cdc + spark + hive + presto + dolphinScheduler + Altas

##### 项目简介：

猫耳APP离线数仓目是以用户为中心搭建的数据分析平台，该项目通过采集、梳理APP用户行为数据和业务数据，并通过spark和hiveSql来对数据进行过滤、解析、集成并入库。该数仓为公司带来了直观的商业价值和意义，解决了营销分析断层、产品迭代无法量化、用户运营不精准、全局运营指标监控不及时等问题，以数据为公司的业务赋能，改善了公司运营情况，提升用户体验。

##### 数据来源：

1. 采集业务系统web端、app端的用户行为日志数据 

2. 抽取注册表、字典表等mysql中的业务表数据 

3. 从第三方/合作方通过数据接口获取外部数据

##### 数据迁移：

1. 日志数据：埋点日志通过flume 组建级联数据传输网络进行拉取，配置taildir、kafkachannel、failover sink processor等组件实现高可用和负载均衡 

2. 业务数据：通过flink-cdc实时获取mysql的binlog数据并写入kafka，并用flume从kafka读取数据后落入hdfs

##### 数仓模块：

对数仓进行分层处理，建立星型模型并进行多维分析 

1. ODS：贴源层，映射原始数据，对数据进行预处理 
2. DWD：进行相关主题表建模，为报表提供数据支撑，数据粒度不变 

3. DWS：以DWD为基础按业务进行主题划分，以轻度聚合表和宽表为主（如用户流量分析主题表，用户日活分析主题表，用户留存分析主题表） 

4. ADS：以 DWD、DWS 为基础，根据实际业务需求生成业务报表(用户行为、事件、促单归因、活动转化、订单统计等) 

##### 数据预处理：

1. 对数据进行清洗过滤和规范化处理，使用session切割策略。

2. 使用账号整合所有用户id，为日志数据添加用户全局唯一标识。 

3. 通过 GeoHash算法 做逆地理位置解析构建参考表，将日志数据中经纬度信息转为 GeoHashcode 关联参考表匹对字典库集成地理位置信息。

##### 报表分析：

1. app流量主题(PV、UV、贡献浏览量、访问时长、用户跳出率分析等) 

2. 用户活跃主题(DAU、DNU、WAU、MAU、用户活跃天数、沉默天数分析等)和用户留存主题（不同渠道留存人数和留存比率、用户留存人数、用户留存比率） 

3. 事件主题(下单转化率、访问路径、广告到达率、各类活动分析) 

4. 归因主题（拉新促单归因、营销渠道贡献率分析等） 
5. 业务主题(用户订单分析、用户产品偏好分析、用户客单价、用户投诉分析等) 

6. 广告主题（点击曝光量分析、点击率分析、人群覆盖率分析、广告转化效果分析） 。

**任务调度系统：** 

使用Dolphin Scheduler调度系统，每天进行定时任务脚本的调度，同时引入Atlas进行元数据管理，对表数据进行血缘关系追溯和表分类管理。

**OLAP报表可视化：** 

基于公司web系统进行报表可视化，引入presto构建多维立方体和即席查询分析。

**项目技术亮点:** 

1. 匿名访问记录、多设备用户及多账号同一设备的身份识别 - 使用业务账号作为唯一标识GUID + 多表联合为匿名用户统一标识。

2. 利用geohash算法逆地理位置解析，将经纬度信息进行地域集成。

3. 用户主题中心事实表下游贡献量的计算使用树结构思想，将被访问页面和来源页抽象成多叉树，根据递归深度遍历根节点，解决了计算难的问题。

4. 连续活跃区间表模型的设计借鉴拉链表思想，表结构天然表达用户活跃的连续性，显著提升该主题报表计算效率。

5. 使用spark混编及正则表达式进行归因分析主题模型开发构建。

**项目迭代升级：** 

1. 利用mysql的binlog机制，使用flink-cdc实现业务数据的实时抓取，实现业务拉链表开发，替代原有sqoop批操作具有延时性的弊端 。

2. 日志数据flume级联传输网络使用kafkachannel将数据实时存储到Kafka集群中，供实时数仓数据统计，解决原有MemoryChannel组件具有数据丢失的弊端。



#### 乐电竞离线数仓开发与维护

2020.10 - 2021.05	数仓开发/数据分析

##### 技术架构：

hdfs + yarn + hbase + mysql + flume  + spark + hive + presto + dolphinScheduler + Altas

##### 项目简介：

该项目是乐电竞通过采集APP内用户行为日志数据、注册数据、订单数据以及业务联机数据库和第三方API数据，对数据进行过滤、解析、集成并入库，按照业务需求划分主题与开发报表，为公司提供日常指标汇报、营销系统赋能、提升用户体验、增加营收等支持。

##### 项目流程：

1. 采集APP端用户行为数据，将数据存入HDFS。
2. 将数据导入Hive库中，对原始数据进行清洗过滤，预处理。
3. 根据业务需求确定主题：用户主题、流量主题、事件主题、订单主题。
4. 流量主题下计算PV、UV、访问时长、用户跳出率分析等。
5. 用户主题下计算用户日活、用户周活、用户留存、用户增长等。
6. 事件主题下计算下单转化率、营销活动参与率、每日任务完成率等。
7. 订单主题下计算每日奖励发放金额、每日提现金额、不同品类订单量、订单金额等。
8. 编写 shell 脚本，使用 Dolphinscheduler 进行任务调度。
9. T-1日报表可视化展示。



#### 腾讯欢乐棋牌生态构建推广项目

2021.10 - 2022.03   PM&数据分析

- **职责:** 负责项目整体对接和统筹、活动策划、对外合作，对内资源分配、款项流程、数据采集清洗整理计算分析、报表指标完成度日统计&周统计。
- **技术栈:** Hive、Spark、Mysql、Kafka
- **成果:** 欢乐棋牌直播平台日最高热度过百万，短视频平台日播放量最高热度过千万，产出头部主播10+，整体超额完成甲方指标，整个项目为公司带来千万级别利润。

#### 腾讯和平精英生态长期合作项目

2021.05 - 2022.05    数据分析

- **职责:** 负责每日收集分析各平台（虎牙斗鱼抖音快手）和平精英板块的新老主播数据，比如粉丝增长、人气、礼物流水、贵宾数、工会信息、直播时长、时间段进行统计，对于新主播，可以为我们的星探招募提供数据支持。对于已经加入生态的老主播，可以监控其直播数据和内容质量，指导主播直播内容运营、参与活动、商业投放等等。
- **技术栈:** Flume、Excel、MySQL
- **成果:** 提高头部主播的曝光和产出、提升新主播招募质量、优化运营侧对主播的培养成本。



## 评价

- 注重代码可读性，有代码洁癖，习惯保持高质量代码和提交记录
- 热爱探索新技术，学习能力强
- 性格活泼易沟通，情绪稳定
- 抗压能力强

